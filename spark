spark3:
-gpu
- cipher as sql for graph network
- graphx graph processing
- python spark streaming
- deltalake as datalake support for amazon s3 etc
- dataframe based mllib instead of rdd based mllib
 - kubernetes integration

RDD:
- sc.textFile() [ 's3:///','file:///','hdfs:///']
- sc.parallelize([1,2,3,4])
- hivectx = HiveContext(sc) -> hivectx.sql('SELECT * FROM x')
- can also be created from cassandra, jdbc, hbase, elastic search, json, csv, object files etc

Basic rdd functions:
- collect(), count(), take(), countByValue(), top(), reduce()
- map()[ex: rdd.map(lambda x : x**2)], flatMap(),filter(),distinct(),union(),intersections(),cartesian(),subtract()

Basic script:
from pyspark import SparkConf, SparkContext
import collections

conf = SparkConf.setMaster('local').setAppname('RatingsHistogram')
sc = SparkContext(conf=conf)
lines = sc.textFile('file:///textfile_loc')
ratings = lines.map(lambda x: x.split()[2]). [Note: This is note inplace modification, the map function creaters a new rdd so needs to be assigned to a variable]
result = ratings.countByValue()
sortedresults = collections.orderedDict(sorted(results.items()))

Key-value RDD:
- reduceByKey()[rdd.reduceByKey(lambda x,y: x+y)][here x and y are values of each key],groupByKey(),sortByKey(),keys(),values()
- 
- If we are not modifying keys, use mapValues(),flatMapValues() instead of map and flatMap(), to avoid shuffling and speeding up the query.
- 

Filtering:
- rdd.filter(lambda x : x[2]=='MIN')

FlatMap;
- map for just modifying the existing keys to different version in values, flat map can create more elements from single source[ex: rdd.flatMap(lambda line: line.split())]
SortByValues:
- flip - rdd.map(lambda (x,y) : (y,x))

rdd.lookup(val)

Accumulators

- used to maintain global counter
- sc.accumulator(0)

Caching
- .cache() to cache in memory
- .persist() to store it in the disk

Set Multiple cores
- .setMaster("local[*]").setAppname('test')

PartitionBy

- join(),cogroup(),leftOuterJoin(),rightOuterJoin(),combineByKey(),lookup(),groupByKey()
- .partitionBy(1000)

EMR spark:

- spark-submit --exercutor-memory 1g MovieSimilarities1M.py 260 (to change the default memory from 512MB to 1 Gb)

Troubleshootin of spark on cluster:
- Usually if error is not clear, need to try increasing memory, number of clusters
- on local clusters, ui runs on port 4040
- Try to avoid obscure external python packages, because for spark to understand those libraries,
.py files should be passed through spark-submit and also should setup a step in EMR with pip install packages
- when executer is failing heartbeat etc, try to manage partitions using .partitionby() or increase the machines


Spark-SQL

from pyspark.sql impor SQLContext, Row
hiveCotext = HiveContext(sc)
inputData= spark.read.json(jsonfilepath)
inputData.createOrReplaceTempView('tempname')
myresultdf = hiveContext.sql('SELECT blah FROm blahblah ORDER BY blahblah')
myresltDF.show()
myresultDF.select('field1')
myresultDF.filter(myresultDF('filed1')>100)
myresultDF.groupBy(myresultDF('filed1')).mean()
myresultDF.rdd().map(mapfunction)

dataframe is dataset of row objects

userdefined functions in sparksql
hiveCtx.registerFunction('funcname',lambda func,IntegerType())

sparksql syntax:
instead of config,  import SparkSession
form spark.sql import Row,SparkSession
sp = spark.builder.AppName.getOrCreate()
sp.SparkContext.TextFile('')
sp.createDataFrame(mapped_row_data).cache()
sp.createOrReplaceTempView('temp_people')
s = sp.sql('SELECT * FROM temp_people')
instead of sql:
s.groupBy('age').count().orderBy('age').show()

¶ DataFrame,wwhich is a higher level api is faster than rdd
¶ sparksession.stop() //important


MLLIB:

- vector(sparser or dense), labeledPoint, Rating
- Vectors.desne(float(i))
- mapper_rdd.toDf(col_list)

streaming
-ssc = StreamingContext(sc,1)
- l = ssc.textFileStream('books')
= c= l.flatMap(mapfunc)
- ssc.start() and ssc.awaitTermination(). //important
- window(),reduceByWindow(), reduceByKeyAndWindow()
- updataStateByKey() [for maintaing running count etc]
- new integration of streaming with dataframe


- ss.readStream.text('logs')


GraphX
- only on scala
- useful for network analysis in scala,provided VecgtorRDDand EdgeRDD



DATA CLEANING USING PYSPARK

- Define schema using StructType([StructField('Name'),StringType,True],[StructField(field_name),DataType(),Null_or_not])
- Spark objects are immutable(like rdds) for the purpose of sharing the object across clusters, so no in place operations are allowed,
result of object modifications should be assigned to new objects
- 









spark3:
-gpu
- cipher as sql for graph network
- graphx graph processing
- python spark streaming
- deltalake as datalake support for amazon s3 etc
- dataframe based mllib instead of rdd based mllib
 - kubernetes integration

RDD:
- sc.textFile() [ 's3:///','file:///','hdfs:///']
- sc.parallelize([1,2,3,4])
- hivectx = HiveContext(sc) -> hivectx.sql('SELECT * FROM x')
- can also be created from cassandra, jdbc, hbase, elastic search, json, csv, object files etc

Basic rdd functions:
- collect(), count(), take(), countByValue(), top(), reduce()
- map()[ex: rdd.map(lambda x : x**2)], flatMap(),filter(),distinct(),union(),intersections(),cartesian(),subtract()

Basic script:
from pyspark import SparkConf, SparkContext
import collections

conf = SparkConf.setMaster('local').setAppname('RatingsHistogram')
sc = SparkContext(conf=conf)
lines = sc.textFile('file:///textfile_loc')
ratings = lines.map(lambda x: x.split()[2]). [Note: This is note inplace modification, the map function creaters a new rdd so needs to be assigned to a variable]
result = ratings.countByValue()
sortedresults = collections.orderedDict(sorted(results.items()))

Key-value RDD:
- reduceByKey()[rdd.reduceByKey(lambda x,y: x+y)][here x and y are values of each key],groupByKey(),sortByKey(),keys(),values()
- 
- If we are not modifying keys, use mapValues(),flatMapValues() instead of map and flatMap(), to avoid shuffling and speeding up the query.
- 

Filtering:
- rdd.filter(lambda x : x[2]=='MIN')

FlatMap;
- map for just modifying the existing keys to different version in values, flat map can create more elements from single source[ex: rdd.flatMap(lambda line: line.split())]
SortByValues:
- flip - rdd.map(lambda (x,y) : (y,x))

rdd.lookup(val)

Accumulators

- used to maintain global counter
- sc.accumulator(0)

Caching
- .cache() to cache in memory
- .persist() to store it in the disk

Set Multiple cores
- .setMaster("local[*]").setAppname('test')

PartitionBy

- join(),cogroup(),leftOuterJoin(),rightOuterJoin(),combineByKey(),lookup(),groupByKey()
- .partitionBy(1000)

EMR spark:

- spark-submit --exercutor-memory 1g MovieSimilarities1M.py 260 (to change the default memory from 512MB to 1 Gb)

Troubleshootin of spark on cluster:
- Usually if error is not clear, need to try increasing memory, number of clusters
- on local clusters, ui runs on port 4040
- Try to avoid obscure external python packages, because for spark to understand those libraries,
.py files should be passed through spark-submit and also should setup a step in EMR with pip install packages
- when executer is failing heartbeat etc, try to manage partitions using .partitionby() or increase the machines


Spark-SQL

from pyspark.sql impor SQLContext, Row
hiveCotext = HiveContext(sc)
inputData= spark.read.json(jsonfilepath)
inputData.createOrReplaceTempView('tempname')
myresultdf = hiveContext.sql('SELECT blah FROm blahblah ORDER BY blahblah')
myresltDF.show()
myresultDF.select('field1')
myresultDF.filter(myresultDF('filed1')>100)
myresultDF.groupBy(myresultDF('filed1')).mean()
myresultDF.rdd().map(mapfunction)

dataframe is dataset of row objects

userdefined functions in sparksql
hiveCtx.registerFunction('funcname',lambda func,IntegerType())

sparksql syntax:
instead of config,  import SparkSession
form spark.sql import Row,SparkSession
sp = spark.builder.AppName.getOrCreate()
sp.SparkContext.TextFile('')
sp.createDataFrame(mapped_row_data).cache()
sp.createOrReplaceTempView('temp_people')
s = sp.sql('SELECT * FROM temp_people')
instead of sql:
s.groupBy('age').count().orderBy('age').show()

¶ DataFrame,wwhich is a higher level api is faster than rdd
¶ sparksession.stop() //important


MLLIB:

- vector(sparser or dense), labeledPoint, Rating
- Vectors.desne(float(i))
- mapper_rdd.toDf(col_list)

streaming
-ssc = StreamingContext(sc,1)
- l = ssc.textFileStream('books')
= c= l.flatMap(mapfunc)
- ssc.start() and ssc.awaitTermination(). //important
- window(),reduceByWindow(), reduceByKeyAndWindow()
- updataStateByKey() [for maintaing running count etc]
- new integration of streaming with dataframe


- ss.readStream.text('logs')


GraphX
- only on scala
- useful for network analysis in scala,provided VecgtorRDDand EdgeRDD



DATA CLEANING USING PYSPARK

- Define schema using StructType([StructField('Name'),StringType,True],[StructField(field_name),DataType(),Null_or_not])
- Spark objects are immutable(like rdds) for the purpose of sharing the object across clusters, so no in place operations are allowed,
result of object modifications should be assigned to new objects
- df.explain()
- df.drop(df(col_name))
- spark.read.format('parquet').load()
- df.write.format('parquet').save('file_name')
- df.filter(df.data>'1/1/2019') or df.where()
- df.select('col_name')
- df.withColumn('new_col',df.new_col)
- df.withrenamedColumn(.,.)
- df.filter(df.col.isNotNull())
- df.where(~ df.col.isNull())
- df.where(df.col.contains('col_value'))
- import pyspark.sql.functions as F [F.lower(),F.split('col_name','-'),F.col.cast(IntegerType())]
- with select and withcolumn can directly pass column name to F.udf() inside arguments instead of passing df['col_name']
- ArrayType().size(col), ArrayType().getItem(index)
- func_var = F.udf(func,resturnType()) => funcVar(df.column)
- pyspark.sql.functions.monotonically_increasing_id(). || voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())
- voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)
- voter_df_single.rdd.getNumPartitions()


Improving performance:
- .cache() // stop caching after finished //df.cache().count()
- .is_cached()
- .unpersist() [to uncache]
- driver process/ worker process
- have well defined schema to spead up file read
- many small files are better than having one big file
- use. wild cards in import sp.read.csv('*.csv')
- spark.conf.set('configuration_name') // spark.conf.get('configuration name')
- Cluster Types - 1) single node 2) standalone(with separate drivers etc), 3) managed(yarn, kubernetes, mesos)
- driver program responsible for shared code access, consolidating results, task assignment. assign twice memory to driver than worker node
- worker runs tasks assigned by driver. more worker nodes is better than larger nodes. balance with costs. fast ssd local storage is better.
- use coalesce(number_of_repartitions) instead of repartition(number of partitions)
- .join often does shuffling, to avoidn this you can use broadcast variables[spark.sql.broadcast()] [use when one df is much smaller][joined_df = split_df.join(broadcast(valid_folders_df), "folder")]
- broadcast_df = flights_df.join(broadcast(airports_df),flights_df["Destination Airport"] == airports_df["IATA"] )
- 


Pipelines: 
- pipeline stages - > inputs, transformations, outputs , validations, analysis
- Issues to solve : Multiple values in single col, different delimiters(use sep=','), empty rows(automatically removed), commented lines(pass comment argument,comment='#'), headers(headers=True)
- change stringtypes to intypes using .cast(IntegerType())
- validatind - verify row/column count, data types of coluimn
- validate via join with smaller df
- udfs used for data manipulation are slower, to avoid this sijple calculations should be inline


MLLIB:
- Algos(classification, regression, ), pipeline(constructing, tuning, persistence), feature(extractions, transdformation), utilities(linear algebra, statistics)
- transformer -> estimator -> Evaluator/class_validator
- pipeline().set_stages()
- paramgridbuilder()
MLLIB datatypes:
- local vector - 0 based index and double type values, two types dense and sparse vectors.
- dense vecotr - numpy arrays and python arrays are recognised as dense vectors in spark
- sparse vector contains two arrays, one arrqay of indexes and one array for values, most of the values are zeroes
- sparsevector(4,[0,2],[0.1,0.2]) will produce array [0.1,0,0,0.2]
- LabeledPointr -  a localvector dense or sparse used in supervosed learning containing double numbers as labels of multi class and 0,1 labels for binary class
- Matrices.sparse(row_num,col_num,(row_indices),(col_indices),(actual)values))
- distributed matrix - long row and col indices, double values, stored in rdds. ex: row matrix(rdd of sequence of vectors without row indices),
indexedrow matrix(like row matrix with meaningfull indices), coordinate matrix(IndexedRow(row_index,col_index,value)), block matrix(row_index,col)index,value)
- local matrix - sparse matrix, dense matrix





